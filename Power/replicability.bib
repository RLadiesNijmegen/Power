@article{ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  volume = {2},
  issn = {1549-1676},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  number = {8},
  journal = {PLOS Medicine},
  doi = {10.1371/journal.pmed.0020124},
  author = {Ioannidis, John P. A.},
  month = aug,
  year = {2005},
  keywords = {Meta-analysis,Clinical research design,Finance,Genetic epidemiology,Genetics of disease,Randomized controlled trials,Research design,Schizophrenia},
  pages = {e124},
  file = {K:\\Literature\\Zotero\\storage\\NJN9RZN9\\Ioannidis_2005_Why Most Published Research Findings Are False.pdf;K:\\Literature\\Zotero\\storage\\TDPV9942\\journal.pmed.html}
}

@article{button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  volume = {14},
  copyright = {2013 Nature Publishing Group},
  issn = {1471-0048},
  shorttitle = {Power Failure},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  language = {en},
  number = {5},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn3475},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  month = may,
  year = {2013},
  pages = {365-376},
  file = {K:\\Literature\\Zotero\\storage\\JCZXYTL4\\Button et al_2013_Power failure.pdf;K:\\Literature\\Zotero\\storage\\L8HTP63U\\nrn3475.html}
}

@article{gelman2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  volume = {9},
  issn = {1745-6916},
  shorttitle = {Beyond {{Power Calculations}}},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  language = {en},
  number = {6},
  journal = {Perspectives on Psychological Science},
  doi = {10.1177/1745691614551642},
  author = {Gelman, Andrew and Carlin, John},
  month = nov,
  year = {2014},
  pages = {641-651},
  file = {K:\\Literature\\Zotero\\storage\\IJ7XYYIQ\\Gelman_Carlin_2014_Beyond Power Calculations.pdf}
}

@misc{yong052019,
   author = {Yong, Ed},
   title = {A Waste of 1,000 Research Papers},
   editor = {theatlantic.com},
   month = may,
   year =  {2019},
   url = {https://www.theatlantic.com/science/archive/2019/05/waste-1000-studies/589684/}
 }

@misc{warren102018,
   author = {Warren, Matthew},
   title = {First analysis of pre-registered' studies shows sharp rise in null findings},
   editor = {nature.com},
   month = oct,
   year =  {2018},
   url = {https://www.nature.com/articles/d41586-018-07118-1},
   doi = {10.1038/d41586-018-07118-1},
 }

@article{schaefer2019,
title={The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases},
author={Schäfer, Thomas and Schwarz, Marcus A.},	
journal={Frontiers in Psychology},	
volume={10},
pages={813},	
year={2019},  
url={https://www.frontiersin.org/article/10.3389/fpsyg.2019.00813},	
doi={10.3389/fpsyg.2019.00813},
issn={1664-1078},   
abstract={Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.}
}

@article{allenpp2018,
title={Open Science challenges, benefits and tips in early career and beyond},
author={Allen, Christopher and Mehler, David},	
journal={PsyArXiv Preprints},
month = nov,
year = {2018},
url={https://psyarxiv.com/3czyt},	
doi={10.31234/osf.io/3czyt},
abstract={The movement towards open science is an unavoidable consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to afflict those who carry out the research, usually Early Career Researchers (ECRs). Here, we describe key benefits including reputational gains, increased chances of publication and a broader increase in the reliability of research. These are balanced by challenges that we have encountered, and which involve increased costs in terms of flexibility, time and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality and plausibility of research. We review three benefits, three challenges and provide suggestions from the perspective of ECRs for moving towards open science practices.}
}


@article{camerer2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  volume = {2},
  issn = {2397-3374},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1\textendash{}15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516\textendash{}36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  number = {9},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0399-z},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  month = sep,
  year = {2018},
  pages = {637-644}
}